# twitter_bot_nlp_2019

For our final project for a Natural Language Processing class, we designed a Language Model using Keras that learns from a user's tweets and then generates similar langauge. We then use these tweets to create a bot that tweets out things similar to what the users would say. 


## File Inventory
- **binary_classifier**:
  - This folder contains pkl file for trained classfier and fitted count vector
- **generated_text**:
  - This folder contains csv files generated by our trained model. 
- **trained_model**:
  - This folder contains trained lstm models. All the models' weights can be found in shared google drive
- **baseline.py**:
  - This py file computes bigram and generate text.
- **binary_classifier.py**:
  - This py file builds a binary text classifier using actual President Trump's and Congresswoman Alexandria Ocasio-Cortez's tweets.
  - This file will then go through all the file generated_text and calculate an accuracy score for each of them. This results is compiled in to evaluation_result.csv
- **eval.py**:
  - This py file computes perplexity and cosin similarity score for our evaluation. 
- **generate.py**
  - This py file is from homework 1. This is a helper file for generating text.
- **models.py**
  - This py file is our main file. It can be used for training our language model. It can also be used for loading pre-trained weights and generate csv files containg tweets generated by pre-trained model into generated_text folder.
  - For transfer learning, please download file from https://nlp.stanford.edu/projects/glove/, and move glove.twitter.27B.25d.txt to the project folder.
- **parse_tweets.py**
  - This py file parses tweets.
- **twitter_api.py**
  - This py file scraped tweets using twitter api. 
- **generate_text_script.sh**
  - This sh script file helps generate all the csv files in generated_text. 
- **run.sh**
  - This sh script file helps run all the training. 
- **ALL_AOC_TWEETS.txt**
  - This txt file contains 2300 scraped Congresswoman Alexandria Ocasio-Cortez's tweets.
- **ALL_TRUMP_TWEETS.txt**
  - This txt file contains scraped President Trump's tweets.
- **TRUMP_500_TWEETS.txt**
  - This txt file contains 500 scraped President Trump's tweets.
- **TRUMP_2600_TWEETS.txt**
  - This txt file contains 2600 scraped President Trump's tweets.

 
  

  
  
  
## Training Model
Run the following command: `models.py --type train  -tf PARSED_TWEETS_FILE_PATH -wf WEIGHTS_FILE_PATH`
There are a number of optional arguments that you can set: 
- -e: Number of epochs (default is 50)
- -do: Dropout (default is 0.3)
- -ea: Early stopping parameter, to have no early stopping use -1 (default is 0.1)
- -em: Embedding size (default is 100)
- -hs: Number of nodes in dense hidden layers (default is 100)

## Building Model from Pre-Cached Weights 
Run the following command: `models.py --type load  -tf PARSED_TWEETS_FILE_PATH -wf WEIGHTS_FILE_PATH`

## Scraping from Twitter + Using our Parser
There are two steps you'll have to do to get tweets into a suitable format for our model. 
1. Scrape tweets
Setup a config.py using the format from config_format.py. Then, run the following command: `python3 twitter_api.py --extract USER_NAME`

2. Parse tweets
Run the following command: `python3 parse_tweets.py --parse RAW_TWEET_FILE_PATH`
This sets up the tweets in a list of lists. Each tweet's list is of the format [TWEET_ID, TWEET_DATE, TWEET_TIME, TWEET_TEXT]. 

In our parsing, we made some decisions for how to tokenize tweets, for example:

- We added whitespace around most punctuation marks so that they are their own tokens. However, we left conjunctions in tact (it's, I'd.)
- We removed all links (which represent either quote tweets or images.) We replaced these with a MEDIA tag (defaults to "<MEDIA>".)
- We removed all "\n" characters within a tweet and instead added a newline indicator "<NL>" to preseve structure in the recreations. 
- We left in emojis. 


## Transfer Learning

Navigate to https://nlp.stanford.edu/projects/glove/ and download a pretrained Twitter embedding (note: file size is large).
