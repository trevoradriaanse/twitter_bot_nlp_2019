# twitter_bot_nlp_2019

For our final project for a Natural Language Processing class, we designed a Language Model using Keras that learns from a user's tweets and then generates similar langauge. We then use these tweets to create a bot that tweets out things similar to what the users would say. 


## File Inventory
- **binary_classifier**:
  - This folder contains pkl file for trained classfier and fitted count vector
- **generated_text**:
  - This folder contains csv files generated by our trained model using models.py's loading mode. 
- **trained_model**:
  - This folder contains trained lstm models. All the models' weights can be found in shared google drive
- **baseline.py**:
  - This py file computes bigram and generate text.
- **binary_classifier.py**:
  - This py file builds a binary text classifier using actual President Trump's and Congresswoman Alexandria Ocasio-Cortez's tweets.
  - This file will then go through all the file in generated_text and calculate an accuracy score for each of them. This results is compiled in to evaluation_result.csv
- **eval.py**:
  - This py file computes perplexity and cosin similarity score for evaluation. 
- **generate.py**
  - This py file is from homework 1. This is a helper file for generating text.
- **models.py**
  - This py file is our main file. It can be used for training our language model. It can also be used for loading pre-trained weights and generate csv files containg tweets generated by pre-trained model into generated_text folder.
  - For transfer learning, please download file from https://nlp.stanford.edu/projects/glove/, and move glove.twitter.27B.25d.txt to the project folder.
  - After training, it documents it's loss, generated text and parameters into result. csv.
- **result.csv**
  - This csv contains result generated by models.py's training. 
- **parse_tweets.py**
  - This py file parses tweets.
- **twitter_api.py**
  - This py file scraped tweets using twitter api. 
- **generate_text_script.sh**
  - This sh script file helps generate all the csv files in generated_text. 
- **run.sh**
  - This sh script file helps run all the training. 
- **ALL_AOC_TWEETS.txt**
  - This txt file contains 2300 scraped Congresswoman Alexandria Ocasio-Cortez's tweets.
- **ALL_TRUMP_TWEETS.txt**
  - This txt file contains scraped President Trump's tweets.
- **TRUMP_500_TWEETS.txt**
  - This txt file contains 500 scraped President Trump's tweets.
- **TRUMP_2600_TWEETS.txt**
  - This txt file contains 2600 scraped President Trump's tweets.
- **evaluation_result.csv**
  - This csv file contains evaluation results generated by binary_classifier.py.


 
  

  
  
  
## Training Model
Run the following command: `models.py --type train  -tf PARSED_TWEETS_FILE_PATH -wf WEIGHTS_FILE_PATH`
There are a number of optional arguments that you can set: 
- -e: Number of epochs (default is 50)
- -do: Dropout (default is 0.3)
- -ea: Early stopping parameter, to have no early stopping use -1 (default is 0.1)
- -em: Embedding size (default is 100)
- -hs: Number of nodes in dense hidden layers (default is 100)

## Building Model from Pre-Cached Weights 
Run the following command: `models.py --type load  -tf PARSED_TWEETS_FILE_PATH -wf WEIGHTS_FILE_PATH`

## Scraping from Twitter + Using our Parser
There are two steps you'll have to do to get tweets into a suitable format for our model. 
1. Scrape tweets
Setup a config.py using the format from config_format.py. Then, run the following command: `python3 twitter_api.py --extract USER_NAME`

2. Parse tweets
Run the following command: `python3 parse_tweets.py --parse RAW_TWEET_FILE_PATH`
This sets up the tweets in a list of lists. Each tweet's list is of the format [TWEET_ID, TWEET_DATE, TWEET_TIME, TWEET_TEXT]. 

In our parsing, we made some decisions for how to tokenize tweets, for example:

- We added whitespace around most punctuation marks so that they are their own tokens. However, we left conjunctions in tact (it's, I'd.)
- We removed all links (which represent either quote tweets or images.) We replaced these with a MEDIA tag (defaults to "<MEDIA>".)
- We removed all "\n" characters within a tweet and instead added a newline indicator "<NL>" to preseve structure in the recreations. 
- We left in emojis. 


## Transfer Learning

Navigate to https://nlp.stanford.edu/projects/glove/ and download a pretrained Twitter embedding (note: file size is large).

## Binary Classifier Evaluation
### Reference https://stackabuse.com/text-classification-with-python-and-scikit-learn/
Here we trained a binary classifier by using sklearn's MultinomialNB as our classifier and both Trump and Alexandria Ocasio-Cortez's actual tweets as our training set. We first converted the tweets into numerical features using bag of words model. Then we transformed the features using TFIDF which solves the issue that the bag of words model does not take into account that certain words might have high occurance in other texts. We achieved a 0.92 accuracy. We saved the model and CountVectorizer for faster evaluation. (Warning: different version of sklearn might result in failure of loading pickle file. In this case, delete the pickle file and redo the training.)

Then the script will loop all the files in the generated text folder, and evaluate all the result files by classifying whether the tweet is AOC's or Trump's. An accuracy score is calculated by wrong classification divided by size of genertated text file,in this case 100. All the results is automatically compiled into evaluation_result.csv. 

